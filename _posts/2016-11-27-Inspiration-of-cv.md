---
title: "听商汤CV报告有感"
tags:
  - CV
  - 感悟
---

这次主要听了来自港中文三个教授各自基于深度学习的讲座，主要内容及感悟如下：

### 数据与知识联合确定的视觉语义感知（Pro.Lin Liang）

林教授首先希望能够将深度学习的黑盒子变成白盒子，加强模型的可解释性。其介绍的工作主要有以下三点：

1. 结构化深度表达学习：从对场景的语言描述（包含语法信息）中挖掘场景的信息，帮助理解场景，CNN（实体类别，位置），RNN（实体间的关系，场景结构配置）
2. 一般化相似度量学习：识别不同来源下同一个人物，主要方法是提出一种图像之间的距离，根据提取出的人物图像的距离判断是否为同一个人，并建立了Hash码对应，在Hamming空间中进行计算，提升算法速度。
3. 自主性样本选择学习：降低对数据标注的依赖，即使用尽量少的有标记的数据，具体方法没怎么懂，不过这个方法值得mark，感觉可以推广到其他问题中。

### 超分辨率图像的内部相似性

吕教授讲座十分风趣幽默而且引人入胜，主要讲了对低分辨率的图像进行高分辨率的重建，具体技术不太懂，github上有该技术导出的项目[waifu2x](https://github.com/nagadomi/waifu2x)，目前还未试用过。吕教授还讲了利用人脸五官位置的先验信息和人脸对齐得到人脸幻构，这部分不怎么懂。

### 深度学习关系建模

林达华教授将学习分为两种：任务导向（task-oriented，如深度学习）和模型导向（model-oriented，如贝叶斯网络，马尔科夫随机场）,林教授希望将这两种模型融合起来，deep network的特点是great expressive power,high performance, but task specific,而graphical model的特点时generatic.（这里老师提到贝叶斯网络主要描述因果关系和时序关系，而马尔科夫随机场主要描述空间关系和互动关系，这两个模型还不熟，不太理解，先记在这里吧。）

从数学的角度来看：deep network是多步的可导的计算过程，受参数控制，而inference: an iterative computational percedure

将概率图的连接分散来看，看成network之间的连接，在利用embedding(encoder and decoder)的方法减少图中节点，即神经元的数目，在进行学习。

### 汤晓鸥教授的讲话

汤老师的讲话也非常幽默，我的收获大约就一点吧，深度学习确实是一个好工具，但若想很好的应用到某领域的话，在该领域中足够的基础和积淀是必须的。so，目前还是踏实学习，踏实看书，踏实看论文吧。
